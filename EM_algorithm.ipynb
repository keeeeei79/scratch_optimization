{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f52d38",
   "metadata": {},
   "source": [
    "# Jensenの不等式\n",
    "- 上に凸の関数なのか下に凸の関数なのかで不等号の向きが変わるので注意\n",
    "- ここでは対数尤度関数を考えるので上に凸の関数の場合は記述する\n",
    "$$\n",
    "    f(\\sum_i \\lambda_i x_i) \\ge \\sum_i \\lambda_i f(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3171b57",
   "metadata": {},
   "source": [
    "# EMアルゴリズム\n",
    "- EMアルゴリズムは観測不可能な潜在変数にモデルが依存する場合の最尤推定の計算方法の1つ\n",
    "- 各変数の定義は以下\n",
    "$$\n",
    "    X: 観測変数, Z: 潜在変数, \\theta: パラメータ, 対数尤度関数(LL) : \\ln p(x|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d365aad",
   "metadata": {},
   "source": [
    "LLに潜在変数を導入する。\n",
    "$$\n",
    "    LL = \\log \\sum_z p(X,Z|\\theta)\n",
    "$$\n",
    "ここに潜在変数についての分布q(Z)を導入し、LLを変分下限とKLに分解できるようにする。変分下限($L(q, \\theta)$)はイェンセンの不等式から導出する。\n",
    "\n",
    "$$\n",
    "    \\log \\sum_z p(X,Z|\\theta) = \\log \\sum_Z q(Z) \\frac{p(X, Z| \\theta)}{q(Z)} \\ge \\sum_Z q(Z) \\log \\frac{p(X, Z| \\theta)}{q(Z)} = L(q, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f86d8d",
   "metadata": {},
   "source": [
    "次にLLと変分下限の差を計算するとLLと変分下限の差がカルバックライブラー情報量になってることがわかる\n",
    "$$\n",
    "    LL - L(q, \\theta) = \\log p(X|\\theta) - \\sum_Z q(Z) \\log \\frac{p(X, Z| \\theta)}{q(Z)} \\\\\n",
    "    = \\log p(X|\\theta) \\sum_Z q(Z) - \\sum_Z q(Z) \\log \\frac{p(Z|X, \\theta)p(X|\\theta)}{q(Z)} \\\\\n",
    "    = \\sum_Z q(Z) \\log p(X|\\theta)  - \\sum_Z q(Z) \\log \\frac{p(Z|X, \\theta)p(X|\\theta)}{q(Z)} \\\\\n",
    "    = \\sum_Z q(Z) \\log \\frac{p(Z|X, \\theta)}{q(Z)} = KL(q, p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ca937",
   "metadata": {},
   "source": [
    "よって対数尤度は変分下限とKLの和に分解できることがわかる\n",
    "$$\n",
    "    LL = L(q, \\theta) + KL(q, p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed66f0",
   "metadata": {},
   "source": [
    "EMアルゴリズムではEステップで$\\theta$を固定し$KL(q,p)=0$となる$q(Z)$を求め、Mステップで$q(Z)$を固定して変分下限$L(q, \\theta)$を最大化する$\\theta$を求める"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006afed2",
   "metadata": {},
   "source": [
    "Eステップ: $KL(q, p)= 0$ となるのは$q=p$となる時なので\n",
    "$$\n",
    "    q(Z) = p(Z|X, \\theta^{old})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78a77e",
   "metadata": {},
   "source": [
    "Mステップ: Eステップで求めた$q(Z)$を使って変分下限を最大化する$\\theta$を求める\n",
    "$$\n",
    "    L(q, \\theta) = \\sum_Z q(Z) \\log \\frac{p(X, Z| \\theta)}{q(Z)} = \\sum_Z p(Z|X, \\theta^{old}) \\log \\frac{p(X, Z| \\theta)}{p(Z|X, \\theta^{old})} \\\\\n",
    "    = \\sum_Z p(Z|X, \\theta^{old}) \\log p(X, Z| \\theta) - p(Z|X, \\theta^{old})p(Z|X, \\theta^{old}) \n",
    "$$\n",
    "ここで$p(Z|X, \\theta^{old})p(Z|X, \\theta^{old})$は定数なのでconstとする。\n",
    "$$\n",
    "    = \\sum_Z p(Z|X, \\theta^{old}) \\log p(X, Z| \\theta) + const = Q(\\theta, \\theta^{old}) + const\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e4c89",
   "metadata": {},
   "source": [
    "あとはここで定義した$Q(\\theta, \\theta^{old})$を最大化する$\\theta$を求める。(EMアルゴリズムでは$p(X, Z| \\theta)$は最適化可能と仮定する)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e546583",
   "metadata": {},
   "source": [
    "# 実際にGMMを計算式に従って実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec525ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
